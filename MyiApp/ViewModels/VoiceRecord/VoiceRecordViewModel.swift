//
//  VoiceRecordViewModel.swift
//  MyiApp
//
//  Created by ìµœë²”ìˆ˜ on 2025-05-08.
//

import Foundation
import CoreML
import Combine
import AVFoundation
import Accelerate

enum CryAnalysisStep {
    case start
    case recording
    case processing
    case result(EmotionResult)
    case error(String)
}

class VoiceRecordViewModel: ObservableObject {

    // MARK: Constants
    private let fftBarCount = 8
    private let fftNormalizationFactor: Float = 5000.0
    private let mfccTargetSampleCount = 15600
    private let outputSampleRate: Double = 22050.0
    private let recordingDuration: TimeInterval = 7.0 // ë…¹ìŒ ì‹œê°„ (7ì´ˆ)

    // MARK: Published Properties
    @Published var audioLevels: [Float] = Array(repeating: 0.0, count: 8) // EqualizerViewì˜ ë§‰ëŒ€ ìˆ˜
    @Published var step: CryAnalysisStep = .start
    @Published var recordingProgress: Double = 0.0

    // MARK: Audio Components
    private var engine: AVAudioEngine?
    private var inputNode: AVAudioInputNode?
    private var audioSession: AVAudioSession?
    private var converter: AVAudioConverter?

    // MARK: Buffers and Timers
    private var recordingBuffer: [Float] = []
    private var analysisTimer: Timer?

    // MARK: CoreML Model
    private let model: DeepInfant_V2 = {
        do {
            return try DeepInfant_V2(configuration: MLModelConfiguration())
        } catch {
            fatalError("âŒ CoreML ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: \(error.localizedDescription)")
        }
    }()

    // MARK: ë‚´ë¶€ ì—ëŸ¬ ì •ì˜
    private enum AudioEngineError: Error {
        case initializationFailed
    }

    // MARK: ì—”ì§„ êµ¬ì„±
    // ì˜¤ë””ì˜¤ ì—”ì§„ ì´ˆê¸°í™” ë° ì…ë ¥ íƒ­ ì„¤ì¹˜
    private func configureEngine(tapHandler: @escaping (AVAudioPCMBuffer, AVAudioTime) -> Void) throws {
        setupAudioSession()
        stopAudioMonitoring()
        
        engine = AVAudioEngine()
        guard let engine = engine else {
            throw AudioEngineError.initializationFailed
        }
        inputNode = engine.inputNode
        recordingBuffer.removeAll()

        let inputFormat = inputNode!.outputFormat(forBus: 0)
        inputNode!.installTap(onBus: 0, bufferSize: 1024, format: inputFormat, block: tapHandler)

        try engine.start()
    }
    
    // MARK: ìœ í‹¸ í•¨ìˆ˜
    // AVAudioPCMBuffer Float ë°°ì—´ ì¶”ì¶œ í•¨ìˆ˜
    func extractFloatArray(from buffer: AVAudioPCMBuffer) -> [Float] {
        guard let channelData = buffer.floatChannelData?[0] else {
            return []
        }
        let frameLength = Int(buffer.frameLength)
        return Array(UnsafeBufferPointer(start: channelData, count: frameLength))
    }
    
    // MARK: ì˜¤ë””ì˜¤ ì„¸ì…˜ ì„¤ì •
    func setupAudioSession() {
        audioSession = AVAudioSession.sharedInstance()
        do {
            try audioSession?.setCategory(.record, mode: .default)
            try audioSession?.setActive(true)
        } catch {
            print("ì˜¤ë””ì˜¤ ì„¸ì…˜ ì„¤ì • ì‹¤íŒ¨: \(error.localizedDescription)")
            DispatchQueue.main.async {
                self.step = .error("ë§ˆì´í¬ ì ‘ê·¼ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤")
            }
        }
    }
    
    // MARK: ë§ˆì´í¬ ê¶Œí•œ í™•ì¸ í•¨ìˆ˜
    func checkMicrophonePermission(completion: @escaping (Bool) -> Void) {
        if #available(iOS 17.0, *) {
            switch AVAudioApplication.shared.recordPermission {
            case .granted:
                completion(true)
            case .denied:
                completion(false)
            case .undetermined:
                AVAudioApplication.requestRecordPermission(completionHandler: { granted in
                    DispatchQueue.main.async {
                        completion(granted)
                    }
                })
            @unknown default:
                completion(false)
            }
        } else {
            switch AVAudioSession.sharedInstance().recordPermission {
            case .granted:
                completion(true)
            case .denied:
                completion(false)
            case .undetermined:
                AVAudioSession.sharedInstance().requestRecordPermission { granted in
                    DispatchQueue.main.async {
                        completion(granted)
                    }
                }
            @unknown default:
                completion(false)
            }
        }
    }
    
    // MARK: ì‹¤ì‹œê°„ ì´í€„ë¼ì´ì €ìš© FFT ë° ë…¹ìŒ ì²˜ë¦¬
    func startAudioMonitoring() {
        checkMicrophonePermission { granted in
            guard granted else {
                DispatchQueue.main.async {
                    self.step = .error("ë§ˆì´í¬ ì ‘ê·¼ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤")
                }
                return
            }

            do {
                try self.configureEngine { buffer, time in
                    // FFT -> ì´í€„ë¼ì´ì € ê°’ ì¶”ì¶œ
                    let magnitudes = self.fftFromBuffer(buffer)
                    DispatchQueue.main.async {
                        self.audioLevels = magnitudes
                    }
                    
                    // ë…¹ìŒ ë²„í¼ì— ë³€í™˜ëœ float ë°ì´í„° ì¶”ê°€
                    guard let converter = self.converter else { return }
                    
                    let inputBlock: AVAudioConverterInputBlock = { _, outStatus in
                        outStatus.pointee = .haveData
                        return buffer
                    }
                    
                    let inputFormat = self.inputNode!.outputFormat(forBus: 0)
                    let outputFormat = AVAudioFormat(commonFormat: .pcmFormatFloat32,
                                                     sampleRate: 22050,
                                                     channels: 1,
                                                     interleaved: false)!
                    
                    let convertedBuffer = AVAudioPCMBuffer(pcmFormat: outputFormat,
                                                           frameCapacity: AVAudioFrameCount(outputFormat.sampleRate) * buffer.frameLength / AVAudioFrameCount(inputFormat.sampleRate))!
                    
                    let status = converter.convert(to: convertedBuffer, error: nil, withInputFrom: inputBlock)
                    if status == .haveData, let floatData = convertedBuffer.floatChannelData?[0] {
                        let newData = Array(UnsafeBufferPointer(start: floatData, count: Int(convertedBuffer.frameLength)))
                        self.recordingBuffer.append(contentsOf: newData)
                    }
                }
                print("âœ… AVAudioEngine ì‹œì‘ë¨")
                self.startRecordingTimer()
                DispatchQueue.main.async {
                    self.step = .recording
                    self.recordingProgress = 0.0
                }
            } catch {
                print("âŒ ì˜¤ë””ì˜¤ ì—”ì§„ êµ¬ì„± ì‹¤íŒ¨: \(error)")
                DispatchQueue.main.async {
                    self.step = .error("ì˜¤ë””ì˜¤ ë…¹ìŒì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                }
            }
        }
    }
    
    // MARK: FFTë¥¼ í†µí•œ ì´í€„ë¼ì´ì € ì• ë‹ˆë©”ì´ì…˜ ê°’ ê³„ì‚°
    private func fftFromBuffer(_ buffer: AVAudioPCMBuffer) -> [Float] {
        let frameCount = Int(buffer.frameLength)
        let log2n = vDSP_Length(log2(Float(frameCount)))
        guard frameCount > 0, let fftSetup = vDSP_create_fftsetup(log2n, FFTRadix(kFFTRadix2)) else {
            return Array(repeating: 0.0, count: fftBarCount)
        }
        defer { vDSP_destroy_fftsetup(fftSetup) }

        guard let channelData = buffer.floatChannelData?[0] else {
            return Array(repeating: 0.0, count: fftBarCount)
        }

        // Hann ìœˆë„ìš° ì ìš© í›„ FFT ìˆ˜í–‰
        var window = [Float](repeating: 0, count: frameCount)
        vDSP_hann_window(&window, vDSP_Length(frameCount), Int32(vDSP_HANN_NORM))
        var windowedSignal = [Float](repeating: 0, count: frameCount)
        vDSP_vmul(channelData, 1, window, 1, &windowedSignal, 1, vDSP_Length(frameCount))

        // ë³µì†Œìˆ˜ ë³€í™˜ ë° magnitude ê³„ì‚°
        var real = [Float](repeating: 0, count: frameCount/2)
        var imag = [Float](repeating: 0, count: frameCount/2)
        var magnitudes = [Float](repeating: 0.0, count: frameCount / 2)
        var normalizedMagnitudes = [Float](repeating: 0.0, count: fftBarCount)

        real.withUnsafeMutableBufferPointer { realPtr in
            imag.withUnsafeMutableBufferPointer { imagPtr in
                var splitComplex = DSPSplitComplex(realp: realPtr.baseAddress!, imagp: imagPtr.baseAddress!)
                windowedSignal.withUnsafeBufferPointer {
                    $0.baseAddress!.withMemoryRebound(to: DSPComplex.self, capacity: frameCount) {
                        vDSP_ctoz($0, 2, &splitComplex, 1, vDSP_Length(frameCount / 2))
                    }
                }
                vDSP_fft_zrip(fftSetup, &splitComplex, 1, log2n, FFTDirection(FFT_FORWARD))
                vDSP_zvmags(&splitComplex, 1, &magnitudes, 1, vDSP_Length(frameCount / 2))
            }
        }

        // êµ¬ê°„ë³„ í‰ê·  -> ë§‰ëŒ€ë³„ ìŠ¤ì¼€ì¼ë§
        let step = magnitudes.count / fftBarCount
        for i in 0..<fftBarCount {
            let start = i * step
            let end = min(start + step, magnitudes.count)
            let slice = magnitudes[start..<end]
            let avg = slice.reduce(0, +) / Float(slice.count)
            normalizedMagnitudes[i] = min(1.0, pow(avg, 0.5) / fftNormalizationFactor)
        }

        return normalizedMagnitudes
    }
    
    // ë…¹ìŒ íƒ€ì´ë¨¸ ì‹œì‘
    private func startRecordingTimer() {
        analysisTimer = Timer.scheduledTimer(withTimeInterval: 0.1, repeats: true) { [weak self] timer in
            guard let self = self else {
                timer.invalidate()
                return
            }
            
            self.recordingProgress += 0.1 / self.recordingDuration
            
            if self.recordingProgress >= 1.0 {
                timer.invalidate()
                self.finishRecording()
            }
        }
    }
    
    // ë…¹ìŒ ì™„ë£Œ ë° ë¶„ì„ ì‹œì‘
    private func finishRecording() {
        stopAudioMonitoring()
        
        DispatchQueue.main.async {
            self.step = .processing
        }
        
        recordAndAnalyzeCry { result in
            DispatchQueue.main.async {
                self.step = .result(result)
            }
        }

        print("ğŸ“ˆ ìµœì¢… ë…¹ìŒëœ ìƒ˜í”Œ ìˆ˜: \(recordingBuffer.count)")
    }
    
    // MFCC [[Float]] -> MLMultiArray ë³€í™˜ í•¨ìˆ˜
    private func createMLMultiArray(from mfcc: [[Float]]) throws -> MLMultiArray {
        let flatArray = mfcc.flatMap { $0 }
        let array = try MLMultiArray(shape: [NSNumber(value: flatArray.count)], dataType: .float32)
        for (i, value) in flatArray.enumerated() {
            array[i] = NSNumber(value: value)
        }
        return array
    }
    
    // MARK: ë¶„ì„ ì‹¤í–‰
    func recordAndAnalyzeCry(completion: @escaping (EmotionResult) -> Void) {
        do {
            try configureEngine { buffer, time in
                let floatArray = self.extractFloatArray(from: buffer)
                self.recordingBuffer.append(contentsOf: floatArray)
            }
            print("ğŸ§ AVAudioEngine ì‹œì‘ë¨")
        } catch {
            print("âŒ ì˜¤ë””ì˜¤ ì—”ì§„ êµ¬ì„± ì‹¤íŒ¨: \(error)")
            step = .error("ì—”ì§„ ì‹œì‘ ì‹¤íŒ¨")
            return
        }

        // ì¼ì • ì‹œê°„ í›„ ë¶„ì„
        DispatchQueue.main.asyncAfter(deadline: .now() + recordingDuration) {
            self.inputNode?.removeTap(onBus: 0)
            self.engine?.stop()
            print("ğŸ™ï¸ ë…¹ìŒ ì™„ë£Œ, ìƒ˜í”Œ ìˆ˜: \(self.recordingBuffer.count)")

            let extractor = MFCCExtractor()
            let mfccFeatures = extractor.extract(from: self.recordingBuffer)
            print("âœ… MFCC ê°œìˆ˜: \(mfccFeatures.count)")
            // Debug prints before checking MFCC validity
            print("ğŸ¯ ì…ë ¥ ê¸¸ì´: \(self.recordingBuffer.count)")
            print("ğŸ“ í”„ë ˆì„ ìˆ˜: \(mfccFeatures.count)")
            if let firstMFCC = mfccFeatures.first {
                print("ğŸ¼ ì²« ë²ˆì§¸ MFCC ë²¡í„°: \(firstMFCC)")
            }

            guard !mfccFeatures.isEmpty else {
                self.step = .error("MFCC ì¶”ì¶œ ì‹¤íŒ¨")
                return
            }


            do {
                // ëª¨ë¸ ì…ë ¥ í¬ê¸° ì¡°ì •(íŒ¨ë”© or ì˜ë¼ë‚´ê¸°)
                let targetCount = self.mfccTargetSampleCount
                let trimmed = Array(self.recordingBuffer.prefix(targetCount)) +
                              Array(repeating: 0.0, count: max(0, targetCount - self.recordingBuffer.count))
                let inputArray = try MLMultiArray(shape: [NSNumber(value: targetCount)], dataType: .float32)
                for (i, value) in trimmed.enumerated() {
                    inputArray[i] = NSNumber(value: value)
                }
                
                // Core ML ëª¨ë¸ ì¶”ë¡ 
                let input = DeepInfant_V2Input(audioSamples: inputArray)
                let output = try self.model.prediction(input: input)
                let label = output.target
                let confidence = output.targetProbability[label] ?? 0.0

                let result = EmotionResult(type: EmotionType(rawValue: label) ?? .unknown, confidence: confidence)
                print("ğŸ” ë¶„ì„ ê²°ê³¼: \(label), ì‹ ë¢°ë„: \(confidence)")
                completion(result)
            } catch {
                print("âŒ CoreML ì¶”ë¡  ì‹¤íŒ¨: \(error.localizedDescription)")
                self.step = .error("ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨")
            }
        }
    }
    
    // MARK: ì •ë¦¬ ë° ì·¨ì†Œ
    func stopAudioMonitoring() {
        analysisTimer?.invalidate()
        analysisTimer = nil
        
        inputNode?.removeTap(onBus: 0)
        engine?.stop()
        engine = nil
        inputNode = nil
    }
    
    func cancel() {
        stopAudioMonitoring()
        step = .start
        recordingProgress = 0.0
    }
    
    func startAnalysis() {
        step = .processing
        print("ğŸŸ¡ ë¶„ì„ ì‹œì‘ë¨")
        
        recordAndAnalyzeCry { result in
            DispatchQueue.main.async {
                self.step = .result(result)
            }
        }
    }
}
